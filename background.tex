\section{Background}
The basic intuition is that reinforcement learning is a form of approximate dynamic programming.
Exact dynamic programs re-use previously computed results through memoization.
In contrast, RL represents the information learned from previously computed results with a learned model.
This model has a particular structure; a regression problem that associates the downstream value (future cumulative cost) with a decision (a join of two relations for a given query).
By projecting the effects of a decision into the future, the model allows us to dramatically prune the search space without exhaustive enumeration.

\subsection{Query Model}
We make the classical assumption of searching for a query plan made up of binary join operators and unary selections, projections and access methods.
We will use the following database of three relations denoting employee salaries as a running example throughout the paper:
\[
\text{Emp}(id, name, rank)~~\text{Pos}(rank, title, code) ~~ \text{Sal}(code, amount)
\]
Consider the following join query:
\begin{lstlisting}
SELECT *
 FROM Emp, Pos, Sal
 WHERE Emp.rank = Pos.rank AND
 Pos.code = Sal.code
\end{lstlisting}
Given this search space, has several possible options on how to execute that query. For example, one could execute the query as $Emp \bowtie (Sal \bowtie Pos)$. Or, one could execute the query as $Sal \bowtie (Emp \bowtie Pos)$. 

\subsection{Graph Model of Enumeration}
To pose the construction of a query plan tree as an RL problem, it will be useful in subsequent discussion by viewing join nesting as a sequence of \emph{graph contractions}. We briefly introduce the formalism here.

\begin{definition}[Query Graph]
Let $G$ define an undirected graph called the \emph{query graph}, where each relation $R$ is a vertex and each $\rho$ defines an edge between vertices. The number of connected components of $G$ are denoted by $\kappa_G$.
\end{definition}

Each possible join is equivalent to a combinatorial operation called a graph contraction.

\begin{definition}[Contraction]
Let $G = (V,E)$ be a query graph with V defining the set of relations and E defining the edges from the join predicates. A contraction $c$ is a function of the graph parametrized by a tuple of vertices $c=(v_i, v_j)$. Applying $c$ to the graph $G$ defines a new graph with the following properties: (1) $v_i$ and $v_j$ are removed from $V$, (2) a new vertex $(v_i+v_j)$ is added to $V$, and (3) the edges of $(v_i+v_j)$ are the union of the edges incident to $v_i$ and $v_j$. 
\end{definition}

Each contraction reduces the number of vertices by $1$. Each plan can be described as a sequence of such contractions $c_1 \circ c_2 ...\circ c_{T}$ until $|V| = \kappa_G$. Going back to our running example, suppose we start with a query graph consisting of the vertices $(Emp, Pos, Sal)$. Let the first contraction be $c_1 = (Emp, Pos)$; this leads to a query graph where the new vertices are $(Emp+Pos, Sal)$. Applying the only remaining possible contraction, we arrive at a single remaining vertex $Sal+(Emp+Pos)$ corresponding to the join plan $Sal \bowtie (Emp \bowtie Pos)$. 

The join optimization problem is to find the best possible one of these contraction sequences---i.e., the best query plan. We have access to a cost model $J$, which is a function that can estimate the incremental cost of a particular contraction $J(c) \mapsto \mathbb{R}_+$. 

\begin{problem}[Join Optimization Problem]
Let $G$ define a query graph and $J$ define a cost model. Find a sequence of $c_1 \circ c_2 ...\circ c_{T}$ terminating in $|V| = \kappa_G$ to minimize:
\[
\min_{c_1,...,c_T} \sum_{i=1}^T J(c_i)
\]\label{joinopt}
\end{problem}

Note how the ``Principle of Optimality'' arises in Problem \ref{joinopt}. Applying each contraction generates a subproblem of a smaller size (a query graph with one less vertex). Each of these subproblems must be solved optimally in any optimal solution. Also note that this model can be simply extended to capture physical operator selection as well. The set of allowed contractions can be typed with an eligible join type e.g., $c=(v_i, v_j, \text{hash\_join})$ or $c=(v_i, v_j, \text{index\_join})$.

\subsection{Greedy vs. Optimal}
A \emph{greedy} solution to this problem is to optimize each $c_i$ independently. The algorithm proceeds as follows: (1) start with the query graph, (2) find the lowest cost contraction, (3) update the query graph and repeat until only one vertex is left. The greedy algorithm, of course, does not consider how local decision might affect future costs. Consider our running example query with the following costs (assume symmetry):
\[J(EP)= 100,~J(SP)= 90,~J((EP)S)= 10,~J((SP)E)= 50\]
The greedy solution would result in a cost of 140 (because it neglects the future effects of a decision), while the optimal solution has a cost of 110.
However, there is an upside: this greedy algorithm has a computational complexity of $O(|V|^3)$, despite the super-exponential search space.

The decision at each index needs to consider the long-term value of its actions where one might have to sacrifice a short term benefit for a long term payoff.
Consider the following way of expressing the optimization problem in the problem statement for a particular query graph $G$:
\begin{equation}
V(G) = \min_{c_1,...,c_T} \sum_{i=1}^T J(c_i)
\label{eq:main}
\end{equation}

 This function is conveniently named the \emph{value function}. Bellman's ``Principle of Optimality'' noted that optimal behavior over an entire decision horizon implies optimal behavior from any starting index $t>1$ as well, which is the basis for the idea of dynamic programming.
So, $V(G)$ can then be defined recursively for any subsequent graph $G'$ generated by future contractions:
\begin{equation}
V(G) = \min_{c}\{~J(c) + \gamma \cdot V(G') ~\}
\label{eq:value}
\end{equation}
It is common to write this value recursion in the following form:
\[
Q(G,c) = J(c) + \gamma \cdot V(G')
\]
Leading to the following recursive definition of the Q-Function (or cost-to-go function):
\begin{equation}
Q(G,c) = J(c) + \gamma \cdot \min_{c'} Q( G',c')
\label{eq:q}
\end{equation}

The Q-Function describes the long-term value of each contraction. 
That means that at each $G'$, local optimization of $\min_{c'} Q(G',c')$ is sufficient to derive an optimal sequence of decisions. Put another way, the Q-function is a hypothetical cost function on which greedy descent is optimal and equivalent to solving the original problem.
If we revisit the greedy algorithm, and revise it as follows: (1) start with the query graph, (2) find the lowest \textbf{Q-value} contraction, (3) update the query graph and repeat. This algorithm has a computational complexity of $O(|V|^3)$ (just like the greedy algorithm) but is provably optimal. 

The Q-Function is one of the main mathematical abstractions in many RL algorithms.
RL algorithms use parametrized models to represent the Q-Function.
However, this is not very different than how enumeration algorithms work today.

In familiar implementations, there is a hash table that maps sets of joined relations to their lowest-cost access path. In Java notation:
\begin{lstlisting}
HashMap<Set<Relation>, (Plan, Long)> bestJoins;
\end{lstlisting}
As the algorithm enumerates more subplans, if a particular relation set exists in the table it replaces a previously-stored access path if the enumerated plan has a lower cost than one in the table. To make this look like a Q-Function, we could equivalently restructure the elements in the table described above. We could consider a set of relations \emph{and} a plan mapping to a cost: 
\begin{lstlisting}
HashMap<(Set<Relation>, Plan), Long> bestQJoins;
\end{lstlisting}
\textbf{TODO: Sanjay will add a description here}

\subsection{Learning the Q-Function}
Reinforcement learning poses the planning problem as a regression problem~\cite{sutton1998reinforcement}.
What if we could regress from features of \texttt{(Set<Relation>, Plan)} to a cost based on a small number of observations?
Instead of a Q-Function as a table, it is parameterized as a model:
\[
Q_\theta(f_G,f_c) \approx Q(G,c)
\]
where $f_G$ is a feature vector representing the query graph and $f_c$ is a feature vector representing the particular contraction on the graph. $\theta$ defines the model parameters that represent this function. 

For those familiar with the AI literature, this problem defines a Markov Decision Process. $G$ is exactly a representation of the \textbf{state} and $c$ is a representation of the \textbf{action}.
The utility function (or reward) of this process is the negative overall runtime.
The objective of the planning problem is to find a policy, which is a map from a query graph to the best possible join $c$.

In the popular Q-Learning approach~\cite{sutton1998reinforcement}, the algorithm enumerates random samples of decision sequences containing $(G,c, runtime, G')$ tuples forming a trajectory. From these tuples, one can calculate the following value:
\[
y_i = J(c) + \arg \max_{c'} Q_\theta(G',c')
\]
Each of the $y_i$ can be used to define a loss function since if $Q$ were the true Q function, then the following recurrence would hold:
\[
Q(G,c) = J(c) + \arg \max_{c'} Q_\theta(G',c')
\]
So, Q-Learning defines a loss:
\[
L(Q) = \sum_{i} \|y_i - Q_\theta(G,c)\|_2^2
\]

This loss can be optimized with gradient descent. When the model being optimized is a deep neural network, this algorithm is called the Deep Q Network algorithm~\cite{mnih2015human} and was used to learn how to autonomously play Atari Games.
The key implication is that the neural network allows for some ability for the optimizer to extroplate the cost-to-go even for plans that are not enumerated. This means that if the featurization $f_G$ and $f_c$ are designed in a sufficiently general way, then the neural network can represent cost-to-go estimates across an entire workload of queries--not just a single query planning instance.
